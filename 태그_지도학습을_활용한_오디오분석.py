# -*- coding: utf-8 -*-
"""태그 지도학습을 활용한_오디오분석.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11a6a_jj1i5WCNXT-1xKX273xy3S0nOBy
"""



# '''
# '''                               _  _
#                                   ||_||
#                                   |'^'|

# '''

!pip install pymysql

# 이거 가져가서 쓰시면 됩니다
# !pip install pymysql
import pymysql
import pandas as pd

host = '34.64.95.29'
port = 3306
user = 'root'
password = 'acorn1234'
database = 'music'

connection = pymysql.connect(host=host, port=port, user=user, password=password, database=database)

Playlist = pd.read_sql_query('SELECT * FROM playlist', connection)
Genre = pd.read_sql_query('SELECT * FROM genre', connection)
Song = pd.read_sql_query('SELECT * FROM song', connection)

connection.close()

Song

Playlist

Genre

# p1 여러가지 노래 (겹치는것, 안겹치는것)
# p2 여러가지 노래 (겹치는것, 안겹치는것)

# 1번작업
# 플레이리스트에는 곡들의 아이디가 존재
# 플레이리스트끼리 중복되는 곡들의 아이디가 많은 것들을 추출해서 데이터프레임 구성  << 협업필터링 데이터풀 구성

# 2번작업
# 중복되지않는부분은 유사도 또는 제목  <<< 여기서 멜스펙트로그램이나, 태그나, 다른, 컨텐츠기반 필터링 추천 방법적용


# 1번작업과 2번작업이 결합된다면 협업필터링과, 컨텐츠기반 필터링이 결합된, 하이브리드 추천시스템 구성가능?

# 네,
# 1번 작업은 협업 필터링을 위한 데이터풀을 구성하는 작업이고,
# 2번 작업은 컨텐츠 기반 필터링을 위한 유사도 계산 또는 제목을 활용한 추천 방법을 적용하는 작업입니다.

# 이 두 작업을 결합하여
# 하이브리드 추천 시스템을 구성할 수 있습니다.
# 하이브리드 추천 시스템은
# 여러 추천 알고리즘을 조합하여,
# 상대적으로, 더 정확하고 다양한 추천을 제공하는 방식입니다.

# 예를 들어,
# 플레이리스트 간의 중복된 곡들을 바탕으로
# 협업 필터링을 적용하여 비슷한 취향을 가진 사용자들에게 추천을 제공하고,
# 중복되지 않은 곡들에 대해서는
# 컨텐츠 기반 필터링을 적용하여
# 유사도를 계산하거나 제목을 활용하여 추천할 수 있습니다.

# 이렇게 협업 필터링과 컨텐츠 기반 필터링을 결합하여
# 사용자의 취향과 컨텐츠의 유사도를 모두 고려하는 추천 시스템을 구성할 수 있습니다.
# 이를 통해 더 나은 추천 결과를 제공할 수 있을 것입니다.

#
# songs 열에서 노래 아이디 개수 계산
song_counts = Playlist['songs'].apply(lambda x: len(x))

# 노래 아이디 개수가 가장 많은 상위 300개 선택
top_songs = Playlist.loc[song_counts.nlargest(300).index]

# 새로운 데이터프레임 구성
new_df_300 = pd.DataFrame(top_songs)

print(new_df_300)

# '''
#  '''                               _  _
#                                   ||_||
#                                   |'^'|

# '''

# 플레이리스트 아이디별로 중복된 노래 아이디 개수 계산
playlist_duplicates = {}
for idx, row in new_df_300.iterrows():
    playlist_id = row['plylst_id']
    song_ids = set(row['songs'].split(', '))
    duplicate_count = 0
    for compare_idx, compare_row in new_df_300.iterrows():
        if compare_idx != idx:
            compare_song_ids = set(compare_row['songs'].split(', '))
            duplicate_count += len(song_ids & compare_song_ids)
    playlist_duplicates[playlist_id] = duplicate_count

# 중복 개수가 가장 많은 플레이리스트 아이디 100개 추출
top_playlists = sorted(playlist_duplicates, key=playlist_duplicates.get, reverse=True)[:100]

# 새로운 데이터프레임 생성
new_df = pd.DataFrame({'Playlist ID': top_playlists, 'Duplicate Count': [playlist_duplicates[i] for i in top_playlists]})

print(new_df)

# 상위 5개의 플레이리스트 아이디 추출(리스트) <<< 노래가 중복된 것의 개수가 가장많은 상위 5개

top_playlists = new_df['Playlist ID'].head(5)
playlist_lists = [[playlist_id] for playlist_id in top_playlists]
playlist_names = ['Playlist {}'.format(i) for i in range(1, 6)]

playlist_dict = dict(zip(playlist_names, playlist_lists))
print(playlist_dict)

playlist_dict

# 'Playlist 1' 리스트에서 플레이리스트 아이디 가져오기
playlist_ids = [int(pid) for pid in playlist_dict['Playlist 1']]

playlist_ids

# 'Playlist 2' 리스트에서 플레이리스트 아이디 가져오기
playlist_ids_2 = [int(pid) for pid in playlist_dict['Playlist 2']]

# 'Playlist 1' 리스트에서 플레이리스트 아이디 가져오기
playlist_ids = playlist_dict['Playlist 1']
filtered_df = Playlist[Playlist['plylst_id'].isin(playlist_ids)]

print(filtered_df)

# 'Playlist 2' 리스트에서 플레이리스트 아이디 가져오기
filtered_df_2 = Playlist[Playlist['plylst_id'].isin(playlist_ids_2)]

print(filtered_df_2)

filtered_df

# Get the list of songs as strings
song_ids_strings = filtered_df.loc[90170, 'songs']

# Convert each string in the list to an integer
song_ids = [int(song_id) for song_id in song_ids_strings]

# Get the song IDs as a string
selected_column = filtered_df.loc[90170, 'songs']

# Split the string into a list using comma as the separator
string_list = selected_column.split(',')

# Remove single quotes from each element in the list
comma_separated_list = [elem.replace("'", "") for elem in string_list]

# Remove leading/trailing whitespace from each element in the list
result_list = [elem.strip() for elem in comma_separated_list]

# Convert the elements in the list to integers
id = [int(elem) for elem in result_list]

id

check_df = Song[Song['song_id'].isin(id)][['song_id','artist_name','song_name']]
check_df

Song

from google.cloud import storage
import numpy as np
import io

# Create a client
client = storage.Client()

# Define bucket name
bucket_name = "apot"

# Get the bucket
bucket = client.get_bucket(bucket_name)

# Get the list of songs
songs = id

# npy파일들을 가져올 apot의 하위디렉토리
path= "extracted_npy_files/"

# Create an empty list to store the loaded npy files
npy_files = []

for song_id in songs:
    # Define the blob name
    blob_name = path + str(song_id) + ".npy"

    # Get the blob
    blob = bucket.blob(blob_name)

    # Download the blob to a bytes object and load it to a numpy array
    blob_bytes = blob.download_as_bytes()
    npy_file = np.load(io.BytesIO(blob_bytes))

    # Append the npy file to the list
    npy_files.append(npy_file)

import librosa
from concurrent.futures import ThreadPoolExecutor
import numpy as np

chroma_data_list = []
with ThreadPoolExecutor(max_workers=5) as executor:
    # Calculate chroma features for each npy file
    for npy_data in npy_files:
        # Compute STFT
        n_fft = min(2048, len(npy_data))  # Set n_fft. Use the smaller of the length of the input signal and 2048
        D = librosa.stft(npy_data, n_fft=n_fft)

        # Extract chroma features
        chroma = librosa.feature.chroma_stft(S=np.abs(D), sr=44100)

        chroma_data_list.append(chroma)

# Check the length of each data and unify them based on the data with the smallest length
min_len = min(chroma.shape[2] for chroma in chroma_data_list)
chroma_data_list = [chroma[:, :, :min_len] for chroma in chroma_data_list]

# Combine chroma features data into a single array
user_combined_data = np.concatenate(chroma_data_list, axis=0)

user_combined_data

import io

def save_combined_npy(bucket, output_dir, filename, data):
    # Save combined data to a npy file
    np.save(filename, data)

    # Create a blob
    blob = bucket.blob(output_dir + filename)

    # Upload the npy file to the blob
    with open(filename, 'rb') as file_obj:
        blob.upload_from_file(file_obj)



# The filename of the npy file to be saved
filename = 'user_combined.npy'

save_combined_npy(bucket, output_dir, filename, combined_data)









filtered_df_2

# 유저가 플레이리스트를 구성



# '''
# '''                               _  _
#                                   ||_||
#                                   |'^'|

# '''



# 중복되는 것, 중복되지 않는 것 구분



# 중복되는 노래 아이디와 중복되지 않는 노래 아이디를 구분하여 새로운 열로 추가 <<< filtered_df_1 을 구분
filtered_df['Duplicate'] = filtered_df.apply(lambda row: [song_id for song_id in row['songs'].split(', ') if song_id in filtered_df_2['songs'].str.split(', ').explode().tolist()], axis=1)
filtered_df['Non-duplicate'] = filtered_df.apply(lambda row: [song_id for song_id in row['songs'].split(', ') if song_id not in filtered_df_2['songs'].str.split(', ').explode().tolist()], axis=1)

filtered_df

# 중복되는 노래 아이디와 중복되지 않는 노래 아이디를 구분하여 새로운 열로 추가 <<< filtered_df_2 을 구분
filtered_df_2['Duplicate'] = filtered_df_2.apply(lambda row: [song_id for song_id in row['songs'].split(', ') if song_id in filtered_df['songs'].str.split(', ').explode().tolist()], axis=1)
filtered_df_2['Non-duplicate'] = filtered_df_2.apply(lambda row: [song_id for song_id in row['songs'].split(', ') if song_id not in filtered_df['songs'].str.split(', ').explode().tolist()], axis=1)

filtered_df

filtered_df_2

# 중복되지 않는 부분에 특징이 있는지, 아이디를 검색해서 노래를 가져와보쟈

# Non-duplicate 정수형 리스트로 변환(filtered_df)
selected_column = filtered_df['Non-duplicate']
integer_list = selected_column.iloc[0]
integer_list = [int(num) for num in integer_list]
Non_1_list = integer_list
Non_1_list

# Non-duplicate 정수형 리스트로 변환(filtered_df_2)
selected_column_2 = filtered_df_2['Non-duplicate']
integer_list_2 = selected_column_2.iloc[0]
integer_list_2 = [int(num) for num in integer_list_2]
Non_1_list_2 = integer_list_2
Non_1_list_2

# 필요한 데이터프레임으로(Non_1_list)
filtered_Playlist_1 = Song[Song['song_id'].isin(Non_1_list)][['song_id','large_cat','sub_cat','artist_id','artist_name','song_name']]
filtered_Playlist_1

# # Nfiltered_Playlist_1에서 large_cat 리스트로 추출.
# selected_column = filtered_Playlist_1['large_cat']
# string_list = selected_column.astype(str).tolist()
# comma_separated_list = [elem.replace("'", "") for elem in string_list]
# comma_separated_string = ','.join(comma_separated_list)
# pre_result_list = [comma_separated_string]
# result_list = [elem.strip("'") for elem in pre_result_list[0].split(',')]
# # 필요한 데이터프레임으로(Non_1_list_2)
# Genre_name_1 = Genre[Genre['genre_id'].isin(result_list)][['genre_name']]
# Genre_name_1

# 필요한 데이터프레임으로(Non_1_list_2)
filtered_Playlist_2 = Song[Song['song_id'].isin(Non_1_list_2)][['song_id','large_cat','sub_cat','artist_id','artist_name','song_name']]
filtered_Playlist_2

# Nfiltered_Playlist_2에서 large_cat 리스트로 추출.
selected_column_2 = filtered_Playlist_2['large_cat']
string_list_2 = selected_column_2.astype(str).tolist()
comma_separated_list_2 = [elem.replace("'", "") for elem in string_list_2]
comma_separated_string_2 = ','.join(comma_separated_list_2)
pre_result_list_2 = [comma_separated_string_2]
result_list_2 = [elem.strip("'") for elem in pre_result_list_2[0].split(',')]
# 필요한 데이터프레임으로(Non_1_list_2)
Genre_name_2 = Genre[Genre['genre_id'].isin(result_list_2)][['genre_name']]
Genre_name_2

# 중복되지 않은 부분에대해서만, mel-spectrogram data를 불러와서

# 유사성파악하고, 그 추출된 유사성을 어떠한 전체학습된 모델에 집어넣어서 ()

# 그 추출된 유사성을 기준으로 검색된, 노래 아이디를 추천 수행.

# 즉,
# 42038	42038	'GN1600'	'GN1601', 'GN1606'	2621782	'Classic Essay'	Grieg Peer Gynt Suite No1 Op46 I Morning Moo...
# 79623	79623	'GN1600'	'GN1601', 'GN1606'	2621782	'Classic Essay'	Mozart Serenade No13 In G Major K525 `Eine Kl...
# 113120	113120	'GN1600'	'GN1601', 'GN1606'	2621782	'Classic Essay'	Chopin Nocturne No1 In B Flat Minor Op91 쇼팽 ...
# 143610	143610	'GN1600'	'GN1601', 'GN1606'	2621782	'Classic Essay'	Kreisler Schon Rosmarin Op55 크라이슬러 아름다운 로즈마린...
# 244293	244293	'GN1600'	'GN1601', 'GN1606'	2621782	'Classic Essay'	Faure Sicilienne In G Minor Op78 포레 시실리안느 사단...
# 314239	314239	'GN1600'	'GN1601', 'GN1606'	2621782	'Classic Essay'	Chopin Nocturne No19 In E Minor Op721 쇼팽 야상곡...
# 373339	373339	'GN1600'	'GN1601', 'GN1606'	2621782	'Classic Essay'	Satie Gymnopedies No1 Lent Et Douloureux 사티 ...
# 376423	376423	'GN0500', 'GN0300'	'GN0501', 'GN0304', 'GN0505', 'GN0301'	980594	'블루 (BLOO)'	Downtown Baby Piano Ver
# 394723	394723	'GN1600'	'GN1601', 'GN1606'	2621782	'Classic Essay'	Martini Plaisir D`amour 마르티니 사랑의 기쁨
# 446786	446786	'GN1600'	'GN1601', 'GN1606'	2621782	'Classic Essay'	Chopin Waltz No10 In B Minor Op692 쇼팽 왈츠 10번...
# 485001	485001	'GN1600'	'GN1601', 'GN1606'	2621782	'Classic Essay'	Liszt Paganini Etude No3 In G Sharp Minor S14...
# 594840	594840	'GN1600'	'GN1601', 'GN1606'	2621782	'Classic Essay'	TchaikovskyRachmaninov The Sleeping Beauty W...


# 이 12 곡과,


# 59078	59078	'GN9000'		1626521	'아기동요나라'	스와니 강 바다소리
# 147917	147917	'GN9000'		1626521	'아기동요나라'	깊고도 넓고도 모닥불소리
# 157888	157888	'GN9000'		1626521	'아기동요나라'	작은별
# 249883	249883	'GN9000'		1626521	'아기동요나라'	비행기
# 257807	257807	'GN9000'		1626521	'아기동요나라'	울면 안돼 모닥불소리
# 268113	268113	'GN9000'		1626521	'아기동요나라'	비행기 모닥불소리
# 318630	318630	'GN9000'		1626521	'아기동요나라'	비행기 바다소리
# 349715	349715	'GN9000'		1626521	'아기동요나라'	나비야 바다소리
# 377541	377541	'GN9000'		1626521	'아기동요나라'	도레미송 바다소리
# 410557	410557	'GN9000'		1626521	'아기동요나라'	도레미송
# 500980	500980	'GN9000'		1626521	'아기동요나라'	작은별 모닥불소리
# 505220	505220	'GN9000'		1626521	'아기동요나라'	울면 안돼

# 이 12곡의 Mel-spectrogram 의 유사성을 추출하고

# 추출한 유사성을


# 전체학습시킨 모델에 집어넣어서


# 유사성가진 ID를 뽑아내고


# 그 ID를 데이터프레임으로 만들어서 추천을 수행.

# 구글 클라우드에 업로드 하고있으므로



# 일단 코드만 던져놓기.



Non_1_list

for item in Non_1_list:
    print(type(item))

for item in Non_1_list_2:
    print(type(item))

# 하위디렉토리 검색하면 오래걸리네
# 확실히, 복원되면,
# 한 path에 npy파일 몰어넣어놔야..

# #  Non_1_list
# #   - 1. npy파일을 불러오고
# #   - 2. np.load()를 해줘야한다
# # * 1,2번 모두 해줘야한다

# import os
# import numpy as np

# # Google Drive의 npy 파일이 저장되어 있는 경로를 지정해주세요.
# folder_path = '/content/drive/MyDrive/mel_unlock'

# # 각 아이디에 해당하는 npy 파일의 데이터를 저장할 리스트를 생성합니다.
# data_list_1 = []

# # 각 아이디에 해당하는 npy 파일을 불러옵니다.
# for id in Non_1_list:
#     file_path_1 = os.path.join(folder_path, f"{id}.npy")

#     # 파일이 존재하는 경우에만 로드합니다.
#     if os.path.isfile(file_path_1):
#         data_1 = np.load(file_path_1)
#         data_list_1.append(data_1) # 데이터를 리스트에 추가합니다.
#     else:
#         print(f"{file_path_1} does not exist.")

# # 이제 'data_list'는 각 npy 파일의 데이터를 요소로 갖는 리스트입니다.

test_list = [10360,10361,10362,10363,10364]


for item in test_list:
    print(type(item))



# test_list_2 = [1038,1039,1040,1041,1042]


# for item in test_list_2:
#     print(type(item))

# # 여기서 song_name 을 가져와서 데이터프레임을 만들

# song_name_label = Song[Song['song_id'].isin(test_list)][['song_id','song_name']]
# song_name_label

# # 여기서 song_name 을 가져와서 데이터프레임을 만들

# song_name_label_2 = Song[Song['song_id'].isin(test_list_2)][['song_id','song_name']]
# song_name_label_2

!pip uninstall en_vectors_web_lg

!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.1.0/en_core_web_lg-3.1.0.tar.gz

!python -m spacy download en_core_web_lg



import spacy
nlp = spacy.load('en_core_web_lg')



# 첫번째 joy 제거
# 두번째, joyful 추가(결론적으로, joy를 joyful로 대체)
# 세번째, joyful을 joyful의 유사어[contented, shining, clapped, dancing]로 대체
# 네번째  awe를 sorry, admire, respect로, 대체
# 5. contented를 cheerful,pleased 로, 대체
# 6. 일단 단어들을 형용사나, 부사로 변경해보자
# 7. cheerful을 bright, optimistic, energetic, march로 대체
# 8. 감정 범주를 추가하기위해, beautiful, sexually, lovesick, homesick를 추가
# 9. 7번으로 되돌려놓음 (9번 백업)
################################################################################################
# 10. 단어의 개수를 20개로 줄임. 지금은 대체한 단어세팅인데, 대체하기전에는 0.05 였고, 지금은 0.2는 된단말이죠
# 11. 사랑에대한 단어 위아래 두개씩추가 (romantic,affectionate,heartbroken,desperate)
# 12.
#     - 1. STFT 동적설정
#     - 2. np.concateㅁㅁnate()에 투입하기위해서 로드된,  _combined.npy 파일을 동일하게 길이가지도록,
# 정확도가 0.5가 한번에 올라감 <<<loss: 0.4907 - accuracy: 0.7236 - val_loss: 0.4760 - val_accuracy: 0.7265
# ###########################################################################################

# 13. 계절은 아닌거같고 날씨 추가후 정확도떨어짐


# 14.  단어중 유사한 관계가 있다면, 포괄적인 단어로 축약

# 9번 백업
# emotion_words = [
#     # Positive emotions
#     'bright', 'energetic', 'march', 'pleased', 'shining', 'clapped', 'dancing', 'loving', 'hopeful', 'serene', 'blissful', 'excited', 'fascinated',
#     'free', 'passionate', 'euphoric', 'triumphant', 'revealing', 'admirable', 'harmonious', 'victorious', 'liberated',
#     'inspiring', 'ecstatic', 'elated', 'amusing', 'respectful'

#     # Negative emotions
#     'grief-stricken', 'sorrowful', 'despairing', 'enraged', 'jealous', 'regretful', 'lonely', 'ashamed', 'heartbroken',
#     'anguished', 'melancholic', 'fearful', 'tormented', 'gloomy', 'disgusted', 'dreadful', 'desolate', 'envious', 'anxious',
#     'betrayed', 'guilty', 'lost', 'remorseful', 'alienated', 'bitter', 'homesick', 'yearning', 'disappointed', 'woeful', 'apologetic'
# ]

# 11번 백업

# emotion_words = [
#     # Positive emotions
#     'joyful', 'peaceful', 'exciting', 'passionate', 'uplifting', 'energetic', 'loving', 'bright', 'cheerful', 'relaxed',
#     'romantic', 'affectionate',

#     # Negative emotions
#     'sad', 'angry', 'lonely', 'anxious', 'depressed', 'frustrated', 'mournful', 'melancholic', 'distressed',
#     'heartbroken', 'desperate'

#     # Mood_words
#      'peaceful', 'lyric', 'tranquil', 'tense', 'dark', 'lively', 'mysterious', 'dramatic', 'nostalgic'



# ]

















# 모델 사이클 돌리기전에,

# 버킷 갱신작업을 합니다.


# 1. emotion_npy <<< 분류되지 않은, npy파일 모음이므로, npy 파일을 모두 삭제하는 명령어를 수행합니다.
!gsutil -m rm 'gs://apot/emotion_npy/*.npy'

# 2. emotion_combined <<< npy파일이, 단어(범주)에 따라, 하나씩존재하므로, emotion_combined의 하위 디렉토리를 삭제하는 명령어를 수행합니다.
!gsutil -m rm -r 'gs://apot/emotion_combined'













# 위의 노래제목에서 감정 단어 추출 <<< 여기서 song데이터프레임 1000개만뽑아서, 단어 조정하고, 다음단계로 갔으면 되었네 .


import pandas as pd
import spacy

# Spacy 모델 로드
nlp = spacy.load('en_core_web_lg') ## 이것도 한번 다른것으로 돌려볼필요. 예를들어 lg를 sm으로 변경

# 감정 단어 목록 정의
# 감정 단어 목록 정의
# 감정 단어 목록 정의
emotion_words = [
    # Positive emotions
    'joyful', 'peaceful', 'exciting', 'energetic', 'loving', 'cheerful', 'relaxed',
    'romantic', 'affectionate',

    # Negative emotions
    'sad', 'angry', 'lonely', 'anxious', 'depressed', 'frustrated',

    # Mood_words
     'tranquil', 'tense', 'dark', 'lively', 'mysterious', 'dramatic', 'nostalgic'
]



# 감정 단어들을 Spacy Doc 객체로 변환
emotion_words_docs = [nlp(word) for word in emotion_words]

def find_most_similar_emotion_word(song_title):
    song_title_doc = nlp(song_title)

    max_similarity = -1
    most_similar_word = None

    for word in song_title_doc:
        for emotion_word_doc in emotion_words_docs:
            similarity = word.similarity(emotion_word_doc)

            if similarity > max_similarity:
                max_similarity = similarity
                most_similar_word = emotion_word_doc.text

    return most_similar_word


# Create a new DataFrame called Song_sample with the first 10,000 rows from the Song DataFrame
Song_sample = Song.iloc[:5000].copy()

# Apply the find_most_similar_emotion_word function to the 'song_name' column and store the results in the 'song_label' column
Song_sample['song_label'] = Song_sample['song_name'].apply(find_most_similar_emotion_word)


# file_path = "/content/sample_data/song_name_label_sample.csv"  # "your_folder"를 원하는 폴더 이름으로 변경
# Song.to_csv(file_path, index=False)

# 놀라운점 Red Velvet : in out 노래 직접 들어보니, 가사에 '화나게(resentful)' 가 있음

# 이거 좀 놀라운데용                                                                       (첫번째세팅)

# 놀라운점 Red Velvet : in out  의 resentful 이 revelation이 폭로아니였나 ?으로 변경됨 << 가사의 전체적인 문맥을 보면, revelation이 맞음
# 이거 좀 놀라운데용                                                                       (두번째세팅)



# 이 코드는 각 노래 제목의 단어와 모든 감정 단어를 비교하여 가장 유사도가 높은 감정 단어를 찾아 새로운 열에 추가합니다.

# 참고로, 'similarity' 함수는 두 단어의 의미 유사도를 -1에서 1까지의 숫자로 측정합니다.

# 1은 완벽하게 일치하는 경우, -1은 완벽하게 일치하지 않는 경우를 나타냅니다.

# 1. 위의 코드에서는 먼저 감정 단어들을 Spacy의 Doc 객체로 변환하여 저장합니다. 이렇게 하면 유사도 계산 시 빠르게 접근할 수 있습니다.

# 2. 그 다음, 각 노래 제목에 대해 'find_most_similar_emotion_word' 함수를 호출하여 가장 유사한 감정 단어를 찾습니다.
  # - 이 함수는 노래 제목의 모든 단어와 각각의 감정 단어 사이의 유사도를 계산,
  # - 가장 유사도가 높은 감정 단어를 반환합니다.

# 3.  각 노래 제목에 대해 'find_most_similar_emotion_word' 함수를 호출하고 결과를 새로운 열에 추가합니다. 이렇게 하면 각 노래 제목에 가장 유사한 감정 단어가 새로운 열에 추가됩니다.

# # Mel-spectrogram의 npy파일을 CNN에 돌릴때,
# # input_shape를 세팅해줘야하므로,


# # 사이즈 확인이 필요


# import numpy as np

# # .npy 파일을 로드합니다

# # 배열의 형태를 출력합니다
# print(data_1.shape)





# 위에 라벨달아준것을 인코딩하는곳


import pandas as pd
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
import numpy as np
import os
import json





# 레이블 인코딩 (문자 -> 숫자)
le = LabelEncoder()
y_encoded = le.fit_transform(Song_sample['song_label'])

# 경로 설정
save_dir = '/content/drive/MyDrive/label_encoding_sample/'

# 디렉토리가 없을 경우 생성
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# 인코딩된 레이블 저장
np.save(save_dir + 'encoded_labels_sample.npy', y_encoded)

# 클래스의 수를 얻습니다.
num_classes = len(le.classes_)

# 클래스의 수를 JSON 파일에 저장
with open(save_dir + 'num_classes_sample.json', 'w') as f:
    json.dump(num_classes, f)

# 레이블을 one-hot 벡터로 변환
y_one_hot = to_categorical(y_encoded, num_classes)

# one-hot 벡터 변환된 레이블 저장
np.save(save_dir + 'one_hot_labels_sample.npy', y_one_hot)

# 클래스의 수를 JSON 파일에 저장
with open(save_dir + 'num_classes_sample.json', 'w') as f:
    json.dump(num_classes, f)

# 위에 저장한 JSON파일 불러오기(코드)
# with open('num_classes.json', 'r') as f:
#     num_classes = json.load(f)





# 샘플작업중

from google.cloud import storage
from google.colab import auth
import numpy as np
import json



# 감정 단어를 저장하는 딕셔너리를 생성합니다.
label_word_mapping = Song_sample['song_label'].to_dict()


auth.authenticate_user()

# GCS 클라이언트 생성
project_id = 'absolute-cipher-384101'  # 여기에 실제 프로젝트 ID를 입력하세요
client = storage.Client(project=project_id)

# GCS 버킷 선택
bucket_name = 'apot'  # 여기에 실제 버킷 이름을 입력하세요
bucket = client.get_bucket(bucket_name)

# npy 파일들이 저장된 경로
npy_files_path = 'extracted_npy_files/'

# 감정 레이블을 불러옵니다.
labels = np.load('/content/drive/MyDrive/label_encoding_sample/encoded_labels_sample.npy')

# 파일 존재 여부와 레이블을 저장할 딕셔너리를 생성합니다.
results = {}

# 파일이 존재하는지 확인합니다.
for i in range(5000):
    blob_name = npy_files_path + f"{i}.npy"
    blob_obj = storage.Blob(blob_name, bucket)

    # 파일이 존재하지 않는 경우에만 출력합니다.
    if not storage.Blob.exists(blob_obj):
        print(f"{blob_name} does not exist in the bucket.")
        results[blob_name] = 'File does not exist'
    else:
        # 파일이 존재하는 경우, 해당 파일의 레이블을 출력하고 딕셔너리에 저장합니다.
        print(f"Label for {blob_name}: {labels[i]} ({label_word_mapping[labels[i]]})")
        results[blob_name] = int(labels[i])

# 결과를 JSON 파일로 저장합니다.
with open('results_sample.json', 'w') as f:
    json.dump(results, f)

results # 이거 각라벨마다 GCS에서 검색해서 가져오도록해도 괜찮을것같은데,

# 이제 각행마다 GCS에서 npy파일을 가져오도록

# 병렬처리 함수정의

from concurrent.futures import ThreadPoolExecutor

def download_npy_file(filename):
    blob_name = npy_files_path + filename
    blob_obj = storage.Blob(blob_name, bucket)

    if storage.Blob.exists(blob_obj):
        npy_data = blob_obj.download_as_bytes()
        npy_array = np.load(BytesIO(npy_data))
        return npy_array
    else:
        return None

from google.cloud import storage
import numpy as np
import pandas as pd
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor

def download_npy_file(filename):
    blob_name = npy_files_path + filename
    blob_obj = storage.Blob(blob_name, bucket)

    if storage.Blob.exists(blob_obj):
        npy_data = blob_obj.download_as_bytes()
        npy_array = np.load(BytesIO(npy_data))
        return npy_array
    else:
        return None


# GCS 클라이언트 생성
project_id = 'absolute-cipher-384101'  # 여기에 실제 프로젝트 ID를 입력하세요
client = storage.Client(project=project_id)

# GCS 버킷 선택
bucket_name = 'apot'  # 여기에 실제 버킷 이름을 입력하세요
bucket = client.get_bucket(bucket_name)

# GCS에서 npy 파일들이 저장된 경로
npy_files_path = 'extracted_npy_files/'

# 결과 딕셔너리를 데이터프레임으로 변환
df_results = pd.DataFrame.from_dict(results, orient='index', columns=['Label'])

# 파일명 열을 추가 (파일명에서 앞에 디렉토리 경로를 제거)
df_results['Filename'] = df_results.index.str.replace(npy_files_path, '', regex=False)

# 레이블을 매칭된 단어로 바꾸기 (label_word_mapping 딕셔너리 활용)
df_results['Emotion'] = df_results['Label'].map(label_word_mapping)

# 라벨로 그룹화하여 파일명을 리스트로 묶음
grouped_df = df_results.groupby('Emotion')['Filename'].agg(list).reset_index()


# GCS에서 각 npy 파일 읽어오기
df_list = []  # 각각의 데이터프레임을 저장할 리스트를 생성합니다.

for _, row in grouped_df.iterrows():
    emotion = row['Emotion']
    filenames = row['Filename']
    emotion_data = []

    with ThreadPoolExecutor(max_workers=5) as executor:
        emotion_data = list(executor.map(download_npy_file, filenames))

    emotion_data = [data for data in emotion_data if data is not None]

    df_emotion = pd.DataFrame({
        'Emotion': emotion,
        'Filenames': [filenames],
        'NpyData': [emotion_data]
    })
    df_list.append(df_emotion)  # 생성된 데이터프레임을 리스트에 추가합니다.

# 결과를 출력합니다.
for df in df_list:
    print(df)





#######################################################################################################
# 병렬처리 추가해서 밑에코드랑 합침. 밑에코드랑 합치면 그합쳐진 코드의 결과는, 단어에 매칭된 npy파일들
#######################################################################################################

#######################################################################################################
# 작업을 반복할 것이므로, 초기 어떠한 부분에서, GCS의 emotion_npy 경로에 모든 데이터들이 삭제되도록
#######################################################################################################

#######################################################################################################
#  초기 어떠한 부분 <<< 밑에 코드 중간에 추가해야
#######################################################################################################

import os
import shutil

# 저장할 디렉토리
save_dir = '/content/drive/MyDrive/emotion_npy'  # 실제 저장할 디렉토리 경로로 변경해야 합니다.

# 저장할 디렉토리가 존재하는지 확인하고, 없다면 만듭니다.
if not os.path.exists(save_dir):
    os.makedirs(save_dir)
else:
    # 디렉토리에 있는 모든 파일을 삭제합니다.
    for filename in os.listdir(save_dir):
        file_path = os.path.join(save_dir, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'{file_path} 파일 삭제 실패. 이유: {e}')

# 갱신(renew) 작업###############################################################

# 위에서 기존의 데이터를 삭제한 후, 각 행의 Emotion과 NpyData 열 값을 가져와서 저장합니다.
for df in df_list:
    emotion = df['Emotion'].iloc[0]
    npy_data_list = df['NpyData'].iloc[0]

    # 각 NpyData를 파일로 저장합니다.
    for i, npy_data in enumerate(npy_data_list):
        save_path = os.path.join(save_dir, f'{emotion}_{i}.npy')
        np.save(save_path, npy_data)

#######################################################################################

# 각 행의 Emotion과 NpyData 열 값을 가져옵니다.
for df in df_list:
    emotion = df['Emotion'].iloc[0]
    npy_data = df['NpyData'].iloc[0]

    # Emotion과 NpyData 열의 값 출력
    print(f'Emotion: {emotion}')
    print(f'NpyData: {npy_data}\n')

from google.cloud import storage
import numpy as np
import pandas as pd
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor
import os
import shutil
from google.cloud import storage
import numpy as np
import io


def download_npy_file(filename, npy_files_path, bucket):
    blob_name = npy_files_path + filename
    blob_obj = storage.Blob(blob_name, bucket)

    if storage.Blob.exists(blob_obj):
        npy_data = blob_obj.download_as_bytes()
        npy_array = np.load(BytesIO(npy_data))
        return npy_array
    else:
        return None

def load_and_process_npy_files(project_id, bucket_name, npy_files_path, results, label_word_mapping):
    # GCS 클라이언트 생성
    client = storage.Client(project=project_id)

    # GCS 버킷 선택
    bucket = client.get_bucket(bucket_name)

    # 결과 딕셔너리를 데이터프레임으로 변환
    df_results = pd.DataFrame.from_dict(results, orient='index', columns=['Label'])

    # 파일명 열을 추가 (파일명에서 앞에 디렉토리 경로를 제거)
    df_results['Filename'] = df_results.index.str.replace(npy_files_path, '', regex=False)

    # 레이블을 매칭된 단어로 바꾸기 (label_word_mapping 딕셔너리 활용)
    df_results['Emotion'] = df_results['Label'].map(label_word_mapping)

    # 라벨로 그룹화하여 파일명을 리스트로 묶음
    grouped_df = df_results.groupby('Emotion')['Filename'].agg(list).reset_index()

    # GCS에서 각 npy 파일 읽어오기
    df_list = []  # 각각의 데이터프레임을 저장할 리스트를 생성합니다.

    for _, row in grouped_df.iterrows():
        emotion = row['Emotion']
        filenames = row['Filename']
        emotion_data = []

        with ThreadPoolExecutor(max_workers=5) as executor:
            emotion_data = list(executor.map(lambda x: download_npy_file(x, npy_files_path, bucket), filenames))

        emotion_data = [data for data in emotion_data if data is not None]

        df_emotion = pd.DataFrame({
            'Emotion': emotion,
            'Filenames': [filenames],
            'NpyData': [emotion_data]
        })
        df_list.append(df_emotion)  # 생성된 데이터프레임을 리스트에 추가합니다.
    return df_list

def save_npy_file(bucket, save_dir, filename, npy_data):
    save_path = save_dir + filename
    blob = bucket.blob(save_path)

    # Numpy array를 바이트로 변환합니다.
    byte_file = io.BytesIO()
    np.save(byte_file, npy_data)
    byte_file.seek(0)

    # 바이트를 GCS에 업로드합니다.
    blob.upload_from_file(byte_file, content_type='application/octet-stream')

def save_npy_files_to_gcs(df_list, bucket_name, save_dir):
    # GCS 클라이언트 생성
    client = storage.Client()

    # GCS 버킷 선택
    bucket = client.get_bucket(bucket_name)

    # 버킷 내에 저장할 디렉토리가 존재하는지 확인하고, 없다면 만듭니다.
    if not any(blob.name == save_dir and blob.name.endswith('/') for blob in bucket.list_blobs()):
        bucket.blob(save_dir).upload_from_string('')  # GCS에는 실제 폴더 개념이 없으므로 빈 파일을 업로드하여 폴더처럼 사용합니다.

    # 갱신(renew) 작업
    # 각 행의 Emotion과 NpyData 열 값을 가져와서 저장합니다.
    with ThreadPoolExecutor() as executor:
        for df in df_list:
            emotion = df['Emotion'].iloc[0]
            npy_data_list = df['NpyData'].iloc[0]

            # 각 NpyData를 파일로 저장합니다.
            for i, npy_data in enumerate(npy_data_list):
                filename = f'{emotion}_{i}.npy'
                executor.submit(save_npy_file, bucket, save_dir, filename, npy_data)

# 함수를 호출하는 방법은 아래와 같습니다.

project_id = 'absolute-cipher-384101'  # 여기에 실제 프로젝트 ID를 입력하세요
bucket_name = 'apot'  # 여기에 실제 버킷 이름을 입력하세요
npy_files_path = 'extracted_npy_files/'  # npy 파일들이 저장된 GCS 경로
results  # npy 파일과 관련된 결과를 포함하는 딕셔너리
label_word_mapping # 레이블을 매칭된 단어로 바꾸는 딕셔너리

# npy 파일을 다운로드하고 처리하는 함수를 호출합니다.
df_list = load_and_process_npy_files(project_id, bucket_name, npy_files_path, results, label_word_mapping)

# GCS에 npy 파일을 저장하는 함수를 호출합니다.
save_dir = 'emotion_npy/'  # npy 파일들이 저장될 GCS 경로
save_npy_files_to_gcs(df_list, bucket_name, save_dir)

import os
import numpy as np
import librosa
from concurrent.futures import ThreadPoolExecutor
from google.cloud import storage
import io

def save_combined_npy(bucket, save_dir, filename, combined_data):
    save_path = save_dir + filename
    blob = bucket.blob(save_path)

    # Numpy array를 바이트로 변환합니다.
    byte_file = io.BytesIO()
    np.save(byte_file, combined_data)
    byte_file.seek(0)

    # 바이트를 GCS에 업로드합니다.
    blob.upload_from_file(byte_file, content_type='application/octet-stream')

def apply_chroma_and_save(df_list, bucket_name, npy_files_dir):
    # GCS 클라이언트 생성
    client = storage.Client()

    # GCS 버킷 선택
    bucket = client.get_bucket(bucket_name)

    # 버킷 내에 저장할 디렉토리가 존재하는지 확인하고, 없다면 만듭니다.
    if not any(blob.name == npy_files_dir and blob.name.endswith('/') for blob in bucket.list_blobs()):
        bucket.blob(npy_files_dir).upload_from_string('')  # GCS에는 실제 폴더 개념이 없으므로 빈 파일을 업로드하여 폴더처럼 사용합니다.

    # 각 행의 Emotion과 NpyData 열 값을 가져와서 처리합니다.
    for df in df_list:
        emotion = df['Emotion'].iloc[0]
        npy_data_list = df['NpyData'].iloc[0]

        chroma_data_list = []
        with ThreadPoolExecutor(max_workers=5) as executor:
            # 각 npy 파일에 대해 Chroma features를 적용합니다.
            for npy_data in npy_data_list:
                # STFT 계산
                n_fft = min(2048, len(npy_data))  # n_fft를 설정. 입력 신호의 길이와 2048 중 작은 값을 사용
                D = librosa.stft(npy_data, n_fft=n_fft)

                # Chroma features 추출
                chroma = librosa.feature.chroma_stft(S=abs(D), sr=44100)

                chroma_data_list.append(chroma)

            # Chroma features를 적용한 결과를 저장할 디렉토리를 생성합니다.
            output_dir = npy_files_dir + emotion + '/'

            # 각 데이터의 길이를 확인하고, 가장 작은 길이를 가진 데이터를 기준으로 통일합니다.
            min_len = min(chroma.shape[2] for chroma in chroma_data_list)
            chroma_data_list = [chroma[:, :, :min_len] for chroma in chroma_data_list]

            # Chroma features 데이터를 하나의 배열로 합칩니다.
            combined_data = np.concatenate(chroma_data_list, axis=0)





            # 합친 데이터를 npy 파일로 저장합니다.
            filename = f'{emotion}_combined.npy'
            executor.submit(save_combined_npy, bucket, output_dir, filename, combined_data)

npy_files_dir = 'emotion_combined/'
apply_chroma_and_save(df_list, bucket_name, npy_files_dir)

# CD품질의 샘플링 레이트 << 44100
# MP3 파일의 샘플링 레이트 <<
# 1. 44100
# 2. 44800


# sr 조정
# 1. 22050 초기(첫번째 EPOCH) accuracy :
# loss: 2.6142 - accuracy: 0.1819 - val_loss: 2.5858 - val_accuracy: 0.1849
# 2. 44100 (MP3_일반)의 샘플링레이트
#loss: 2.6144 - accuracy: 0.1806 - val_loss: 2.5904 - val_accuracy: 0.1827

# 단어변경후 정확도
# loss: 2.6398 - accuracy: 0.2158 - val_loss: 2.6314 - val_accuracy: 0.2146(단어 개수 줄이니, 초기 accuracy 0.3올라감)


# sr 조절하는 것은 의미가 없어보임

# #import os
# import numpy as np
# import os

# # npy 파일이 저장된 경로
# npy_files_dir = '/content/drive/MyDrive/emotion_npy/chroma_features'

# # 각각의 하위 디렉토리에 대해 _combined.npy 파일을 불러옵니다.
# for emotion_word in os.listdir(npy_files_dir):
#     # 감정 단어에서 하이픈을 언더스코어로 변경합니다.
#     variable_name = emotion_word.replace('-', '_')
#     npy_file_path = os.path.join(npy_files_dir, emotion_word, f'{emotion_word}_combined.npy')

#     # npy 파일이 실제로 존재하는지 확인합니다.
#     if os.path.isfile(npy_file_path):
#         # npy 파일을 로드하여 각각의 감정 단어로 선언된 변수에 저장합니다.
#         globals()[variable_name] = np.load(npy_file_path)

# # 이제 각각의 감정 단어로 선언된 변수에 _combined.npy 파일의 데이터가 저장되어 있습니다.
# # 예를 들어, 'happy' 감정에 해당하는 데이터를 불러오려면 다음과 같이 사용할 수 있습니다:
# # print(happy)

npy_file_path

# admirable.shape

# 1. 1344는 합친 데이터의 총 샘플 수를 나타냅니다.
#      여기서 샘플은 특정 감정에 대해 Chroma 특징을 계산한 결과입니다.


# 2. 12는 각 샘플에 포함된 Chroma 특징 벡터의 수를 나타냅니다.(12개의 음피치 클래스)
#      이는 Chroma 특징이 12개의 음피치 클래스를 사용하여 음악 신호의 피치 내용을 요약하므로, 각 샘플마다 12개의 Chroma 특징 벡터가 있습니다.


# 3. 4는 각 Chroma 특징 벡터에 포함된 값의 수를 나타냅니다.
#      이는 사용한 방법이나 설정에 따라 다르며, 본 경우에는 각 Chroma 특징 벡터가 4개의 값을 가진다고 볼 수 있습니다.







# GCS에서 분류된 폴더 가져오기

!gsutil -m cp -r gs://apot/emotion_combined /content/drive/MyDrive/





import os
import numpy as np
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from keras.optimizers import Adam
from keras.initializers import GlorotUniform
from keras.layers import Dropout

# npy 파일이 저장된 경로
npy_files_dir = '/content/drive/MyDrive/emotion_combined'

data = []
labels = []
max_length = 0
max_length_file = ''

# 각각의 하위 디렉토리에 대해 _combined.npy 파일을 불러옵니다.
for idx, emotion_word in enumerate(os.listdir(npy_files_dir)):
    # 감정 단어에서 하이픈을 언더스코어로 변경합니다.
    variable_name = emotion_word.replace('-', '_')
    npy_file_path = os.path.join(npy_files_dir, emotion_word, f'{emotion_word}_combined.npy')

    # npy 파일이 실제로 존재하는지 확인합니다.
    if os.path.isfile(npy_file_path):
        # npy 파일을 로드합니다.
        loaded_data = np.load(npy_file_path)
        print(f'Loaded {npy_file_path} with shape {loaded_data.shape}')  # 로드된 데이터의 shape를 출력합니다.

        # 현재 로드된 데이터의 길이가 이전 최대 길이보다 크다면, 최대 길이를 업데이트 합니다.
        if loaded_data.shape[2] > max_length:
            max_length = loaded_data.shape[2]
            max_length_file = npy_file_path  # 최대 길이를 가진 파일의 이름을 업데이트

        # 데이터의 차원이 일치하면 data 리스트에 추가합니다.
        data.append(loaded_data)
        # 해당 데이터에 대한 레이블을 생성하고 labels 리스트에 추가합니다.
        labels.extend([idx] * len(data[-1]))
    else:
        print(f'{npy_file_path} does not exist.')  # npy 파일이 존재하지 않는 경우 경고 메시지를 출력합니다.

# 모든 데이터를 max_length에 맞게 조정합니다.
data = [np.pad(loaded_data, ((0, 0), (0, 0), (0, max_length - loaded_data.shape[2]))) for loaded_data in data]

# 최대 길이를 가진 파일의 이름과 그 길이를 출력
print(f'The file with the longest data is {max_length_file} with length {max_length}')

# # 이제 data에 저장된 모든 넘파이 배열이 같은 shape를 가집니다.
# data = np.concatenate(data, axis=0)

# Min-max
data = (data - np.min(data)) / (np.max(data) - np.min(data))


labels = np.array(labels)

# 데이터를 학습 데이터와 테스트 데이터로 분할
train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.3, random_state=42)

# 레이블을 one-hot encoding으로 변환
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

# 클래스의 수를 레이블 벡터의 길이와 동일하게 설정
num_classes = train_labels.shape[1]

# 모델 정의
model = Sequential()
model.add(Conv1D(32, 3, activation='relu', kernel_initializer=GlorotUniform(), input_shape=(12, max_length))) # input_shape에 max_length를 포함
model.add(Dropout(0.5)) # Dropout 레이어 추가, 비율은 50%
model.add(Flatten())
model.add(Dense(32, activation='relu', kernel_initializer=GlorotUniform()))
model.add(Dropout(0.5)) # Dropout 레이어 추가, 비율은 50%
model.add(Dense(num_classes, activation='softmax', kernel_initializer=GlorotUniform()))

# 모델 컴파일
optimizer = Adam(lr=0.001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# 모델 학습
model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(test_data, test_labels))

import os
import numpy as np
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from keras.optimizers import Adam
from keras.initializers import GlorotUniform
from keras.layers import Dropout

# npy 파일이 저장된 경로
npy_files_dir = '/content/drive/MyDrive/emotion_combined'

data = []
labels = []
max_length = 0
max_length_file = ''

# 각각의 하위 디렉토리에 대해 _combined.npy 파일을 불러옵니다.
for idx, emotion_word in enumerate(os.listdir(npy_files_dir)):
    # 감정 단어에서 하이픈을 언더스코어로 변경합니다.
    variable_name = emotion_word.replace('-', '_')
    npy_file_path = os.path.join(npy_files_dir, emotion_word, f'{emotion_word}_combined.npy')

    # npy 파일이 실제로 존재하는지 확인합니다.
    if os.path.isfile(npy_file_path):
        # npy 파일을 로드합니다.
        loaded_data = np.load(npy_file_path)
        print(f'Loaded {npy_file_path} with shape {loaded_data.shape}')  # 로드된 데이터의 shape를 출력합니다.

        # 현재 로드된 데이터의 길이가 이전 최대 길이보다 크다면, 최대 길이를 업데이트 합니다.
        if loaded_data.shape[2] > max_length:
            max_length = loaded_data.shape[2]
            max_length_file = npy_file_path  # 최대 길이를 가진 파일의 이름을 업데이트

        # 데이터의 차원이 일치하면 data 리스트에 추가합니다.
        data.append(loaded_data)
        # 해당 데이터에 대한 레이블을 생성하고 labels 리스트에 추가합니다.
        labels.extend([idx] * len(data[-1]))
    else:
        print(f'{npy_file_path} does not exist.')  # npy 파일이 존재하지 않는 경우 경고 메시지를 출력합니다.

# 최대 길이를 가진 파일의 이름과 그 길이를 출력
print(f'The file with the longest data is {max_length_file} with length {max_length}')

# 모든 데이터를 max_length에 맞게 조정합니다.
data = [np.pad(loaded_data, ((0, 0), (0, 0), (0, max_length - loaded_data.shape[2]))) for loaded_data in data]

# 이제 data에 저장된 모든 넘파이 배열이 같은 shape를 가집니다.
data = np.concatenate(data, axis=0)

# Min-max
data = (data - np.min(data)) / (np.max(data) - np.min(data))

labels = np.array(labels)

# 데이터를 학습 데이터와 테스트 데이터로 분할
train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.3, random_state=42)

# 레이블을 one-hot encoding으로 변환
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

# 클래스의 수를 레이블 벡터의 길이와 동일하게 설정
num_classes = train_labels.shape[1]

# 모델 정의
model = Sequential()
model.add(Conv1D(32, 3, activation='relu', kernel_initializer=GlorotUniform(), input_shape=(12, max_length))) # input_shape에 max_length를 포함
model.add(Dropout(0.5)) # Dropout 레이어 추가, 비율은 50%
model.add(MaxPooling1D(pool_size=2)) # MaxPooling 레이어 추가
model.add(Conv1D(64, 3, activation='relu', kernel_initializer=GlorotUniform())) # Conv1D 레이어 추가
model.add(Dropout(0.5)) # Dropout 레이어 추가, 비율은 50%
model.add(Flatten())
model.add(Dense(64, activation='relu', kernel_initializer=GlorotUniform())) # Dense 레이어 추가
model.add(Dropout(0.5)) # Dropout 레이어 추가, 비율은 50%
model.add(Dense(num_classes, activation='softmax', kernel_initializer=GlorotUniform())) # 마지막 Dense 레이어 추가

# 모델 컴파일
optimizer = Adam(lr=0.0001) # 학습률을 0.0001로 설정
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# 모델 학습
model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(test_data, test_labels))

# 모델 저장
model.save('first_save.h5')

if user_combined_data.shape[2] < max_length:
    user_combined_data = np.pad(user_combined_data, ((0, 0), (0, 0), (0, max_length - user_combined_data.shape[2])))

predictions = model.predict(user_combined_data)

predictions

import numpy as np

# Set print options
np.set_printoptions(threshold=np.inf)

# Print predictions
print(predictions)

# 즉, predictions의 각 행은
# user_combined_data의 해당 행의 입력 데이터가
# 각 클래스에 속할 확률을 나타내는 값들을 담고 있습니다.
# 이 값들 중 가장 큰 값이 해당 입력 데이터의 예측 클래스를 결정합니다.

# 모델 불러오기
from keras.models import load_model
model = load_model('first_save.h5')





# 위 코드에서 노드를 정의하는 부분이있나요

# 제공된 코드에서 "노드"를 정의하는 부분은 Dense 레이어에서 확인할 수 있습니다. 여기서 노드는 신경망에서 개별적인 처리 단위를 의미합니다. Dense 레이어에서 첫 번째 인자는 레이어에 있는 노드(뉴런)의 수를 지정합니다. 아래 코드를 예시로 보겠습니다:

# model.add(Dense(32, activation='relu', kernel_initializer=GlorotUniform()))

# 이 코드에서는 32개의 노드를 가진 완전 연결 계층(Dense layer)를 모델에 추가하고 있습니다. 각 노드는 입력으로부터 가중치를 곱한 합을 계산하고, 활성화 함수를 거쳐서 결과를 출력합니다. 이 경우의 활성화 함수는 ReLU(Rectified Linear Unit)입니다.

# 마찬가지로 아래의 코드:

# model.add(Dense(num_classes, activation='softmax', kernel_initializer=GlorotUniform()))

# 에서는 num_classes 개의 노드를 가진 완전 연결 계층을 추가하고 있습니다. 이 경우 활성화 함수는 softmax로, 다중 클래스 분류 문제에서 출력 레이어에서 주로 사용됩니다. softmax 함수는 각 클래스에 대한 확률을 계산하여 반환합니다.

# 또한, Conv1D 레이어에서 첫 번째 인자도 노드의 수를 나타냅니다. 이 노드들은 합성곱 연산을 수행하는 필터로 간주할 수 있습니다. 이 필터들은 입력 데이터의 지역적 특징을 학습하는 역할을 합니다.

# model.add(Conv1D(64, 3, activation='relu', kernel_initializer=GlorotUniform(), input_shape=(12, max_length)))

# model.add(Conv1D(64, 3, activation='relu', kernel_initializer=GlorotUniform(), input_shape=(12, max_length))) # input_shape에 max_length를 포함
# model.add(MaxPooling1D(pool_size=2))
# model.add(Dropout(0.25)) # Dropout 레이어 추가, 비율은 50% 이 구조의 출력노드는 몇개?



# 본 코드에서의 출력 노드 수는 Conv1D 레이어의 필터 수와 MaxPooling1D의 pool_size에 의해 결정됩니다.

# Conv1D(64, 3, activation='relu', kernel_initializer=GlorotUniform(), input_shape=(12, max_length)): 이 레이어는 합성곱 연산을 수행하며, 각 필터가 입력 데이터와 합성곱을 구해 새로운 특성 맵을 만듭니다. 여기서 필터의 수는 64개입니다. 그러나 필터의 수가 출력 노드의 수를 결정하지는 않습니다. 대신, 각 필터가 만드는 특성 맵의 차원(길이)이 출력의 길이를 결정하게 됩니다.

# MaxPooling1D(pool_size=2): 이 레이어는 특성 맵의 차원을 절반으로 줄입니다. 그러므로 이 연산 후 출력의 길이는 (max_length - 2) / 2 + 1이 됩니다. 이는 필터 크기 3의 Conv1D가 적용된 후 길이가 max_length - 2가 되고, 이후 MaxPooling이 적용되어 길이가 절반으로 줄어들기 때문입니다.

# Dropout(0.25): 이 레이어는 학습 과정에서 무작위로 선택한 노드의 일부(여기서는 25%)를 비활성화시킵니다. 그러나 Dropout 레이어는 출력의 크기를 변경하지 않습니다.

# 따라서, 본 코드 구조에서 출력 노드는 몇 개인지 단정적으로 말하기 어렵습니다. 왜냐하면 출력의 차원은 Conv1D와 MaxPooling1D 레이어에 의해 결정되며, 그 값은 max_length 값에 따라 달라지기 때문입니다. max_length의 정확한 값을 알아야 최종 출력 노드의 수를 계산할 수 있습니다.

# 입력노드는 64이지요?
# 입력 노드의 개수는 모델에 제공되는 특성의 개수에 따라 결정됩니다. 이 경우, 입력 데이터의 형태는 (12, max_length)입니다. 이 중 12는 입력 특성의 개수(즉, 노드 수)이며, max_length는 각 특성의 시퀀스 길이를 나타냅니다. 따라서, 한 번에 하나의 특성 시퀀스를 고려할 때, 입력 노드의 수는 12개가 됩니다.

# 그러나 이를 신경망에 입력으로 제공할 때, 각 특성의 전체 시퀀스(max_length 길이)가 신경망에 공급됩니다. 이 경우, 단일 입력 벡터의 노드 수는 12 x max_length가 될 것입니다.

# 다음의 코드에서,

# model.add(Conv1D(64, 3, activation='relu', kernel_initializer=GlorotUniform(), input_shape=(12, max_length)))

# 64는 첫 번째 합성곱 레이어의 필터 수를 나타내며, 입력 노드의 수와는 관련이 없습니다. 이 필터들은 입력 특성에 적용되어 새로운 특성 맵을 생성합니다.

# 64는 첫 번째 합성곱 레이어의 필터 수를 나타내며, 입력 노드의 수와는 관련이 없습니다. 이 필터들은 입력 특성에 적용되어 새로운 특성 맵을 생성합니다.



























# 질문 <<< Mel-spectrogram 데이터가 주어졌다면, 다음 작업으로 무엇을 할 수 있을까요?


# 답변

# Mel-spectrogram 데이터가 주어졌을 때, 다음과 같은 작업을 수행할 수 있습니다:

# 음악 분류(Classification):
# - Mel-spectrogram 데이터를 사용하여 음악을 여러 장르 또는 카테고리로 분류할 수 있습니다.
# - 머신 러닝 모델을 훈련하여 각 장르 또는 카테고리에 대한 분류기를 구축할 수 있습니다.

# 음악 유사도 측정(Similarity Measurement):
# - Mel-spectrogram 데이터를 사용하여 음악 간의 유사도를 측정할 수 있습니다.
# - 유사한 특징을 가진 Mel-spectrogram을 사용하여 음악 간의 거리나 유사성을 측정하는 방법을 개발할 수 있습니다.

# 음악 추천(Recommendation)
# - Mel-spectrogram 데이터와 사용자의 선호도를 기반으로 음악 추천 시스템을 구축할 수 있습니다.
# - 사용자의 취향과 유사한 음악을 추천하는 모델을 개발하여 개인화된 음악 추천 서비스를 제공할 수 있습니다.

# 음악 생성(Generation):
# - Mel-spectrogram 데이터를 사용하여 새로운 음악을 생성할 수 있습니다.
# - 딥러닝 기반의 생성 모델을 훈련시켜 Mel-spectrogram 데이터를 입력으로 받고,
# - 새로운 멜로디 또는 음악을 생성하는 모델을 구축할 수 있습니다.

# 음악 변환(Transformation):
# - Mel-spectrogram 데이터를 사용하여 음악을 변환할 수 있습니다.
# - 예를 들어, 특정 장르의 음악을 다른 장르의 음악으로 변환하거나,
# - 음악의 속도나 음색을 조정하는 등의 변환 작업을 수행할 수 있습니다.


# 음악의 감정 분석 (Music Emotion Analysis):
# - 음악에는 다양한 감정이 담겨 있을 수 있으며,
# - 예를 들어, [기쁨, 슬픔, 분노, 평온] 등의 감정을 음악으로 표현할 수 있습니다.
# - 감정 분석을 통해 음악을 자동으로 태깅하거나,
# - 음악의 감정적인 특징을 이용하여 감정 기반의 음악 추천 시스템을 개발할 수 있습니다.
# - 음악의 감정 분석은 주로 음악의 멜로디, 리듬, 가사 등을 분석하여 감정적인 특징을 추출하고,
# - 머신 러닝 기법이나 감정 분석 알고리즘을 활용하여 감정을 분류하거나 회귀하는 방식으로 수행됩니다.

# 악기 인식 (Musical Instrument Recognition):
# - 악기 인식은 음악에서 사용되는 악기를 식별하는 작업입니다.
# - 음악은 여러 종류의 악기가 함께 연주되는 경우가 많으며, 악기별로 특정한 소리 특성을 가지고 있습니다.
# - 악기 인식은 음악 신호를 분석하여 어떤 악기가 사용되었는지 식별하는 과정으로 수행됩니다.
# - 이를 위해 주파수, 주파수 영역에서의 특성, 시간 영역에서의 특성 등을 추출하여 분류 모델을 훈련시키는 방식을 사용합니다.
# - 악기 인식은 음악 정보 검색, 음악 교육, 음악 편집 등에 활용될 수 있습니다.

# 화자 인식 (Speaker Recognition):
# - 화자 인식은 음성 신호에서 개별 화자를 식별하는 작업입니다.
# - 음악에는 가수의 목소리가 포함되어 있기 때문에 화자 인식 기술은 음악에서 가수의 목소리를 식별
# - 음악 아티스트의 음악 스타일이나 특징을 파악하는 데 사용될 수 있습니다.
# - 화자 인식은 음성 신호를 특징 벡터로 변환
# - 이를 이용하여 화자의 고유한 특성을 학습하여 인식하는 과정으로 이루어집니다.
# - 주요한 특징은 화자의 목소리 톤, 발음 스타일, 발화 패턴 등입니다.
# - 화자 인식은 음악 정보 검색, 음악 편집, 음악 저작권 관리 등에 활용될 수 있습니다.



